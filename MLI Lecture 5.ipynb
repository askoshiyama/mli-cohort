{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Boston Housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Data and Main Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/askoshiyama/mli-cohort3/master/boston.csv\")\n",
    "df_onevar = df[[\"V6\", \"T1\"]].copy()\n",
    "\n",
    "# pass some parameters\n",
    "target = [\"T1\"]\n",
    "\n",
    "# generate folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting small - Ridge regression - One variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-instantiation\n",
    "df_metrics = pd.DataFrame(index=[0], columns=[\"Fold\", \"Shrinkage\", \"Train RMSE\", \"Test RMSE\"])\n",
    "\n",
    "# main loop\n",
    "k, f = 0, 0\n",
    "for (train, test) in kf.split(df_onevar):\n",
    "    f += 1\n",
    "    # separate variables and folds\n",
    "    x_train = df_onevar.drop(labels=target, axis=1).values[train]\n",
    "    x_test = df_onevar.drop(labels=target, axis=1).values[test]\n",
    "    y_train = df_onevar[target].values[train]\n",
    "    y_test = df_onevar[target].values[test]\n",
    "    \n",
    "    # scale variables\n",
    "    scaler_x = StandardScaler().fit(x_train)\n",
    "    x_train = np.hstack([np.ones((x_train.shape[0], 1)), scaler_x.transform(x_train)])\n",
    "    x_test = np.hstack([np.ones((x_test.shape[0], 1)), scaler_x.transform(x_test)])\n",
    "    \n",
    "    # fit model\n",
    "    # train model\n",
    "    l = 5.0 # ridge shrinkage\n",
    "    inv_component = np.linalg.inv(np.matmul(x_train.transpose(), x_train) + np.eye(x_train.shape[1]) * l * x_train.shape[1])\n",
    "    coefs = np.matmul(inv_component, np.matmul(x_train.transpose(), y_train))\n",
    "\n",
    "    # get predictions\n",
    "    pred_train = np.matmul(x_train, coefs)\n",
    "    pred_test = np.matmul(x_test, coefs)\n",
    "\n",
    "    # compute metrics\n",
    "    rmse_train = np.sqrt(np.mean((y_train - pred_train) ** 2.0))\n",
    "    rmse_test = np.sqrt(np.mean((y_test - pred_test) ** 2.0))\n",
    "\n",
    "    # store results\n",
    "    df_metrics.loc[k, \"Fold\"] = f\n",
    "    df_metrics.loc[k, \"Shrinkage\"] = l\n",
    "    df_metrics.loc[k, \"Train RMSE\"] = rmse_train\n",
    "    df_metrics.loc[k, \"Test RMSE\"] = rmse_test\n",
    "    k += 1\n",
    "\n",
    "# final organization\n",
    "print(coefs)\n",
    "print(df_metrics.astype(float).mean())\n",
    "plt.scatter(x_train[:, 1], y_train)\n",
    "plt.plot(x_train[:, 1], pred_train, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model - All Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-instantiation\n",
    "df_metrics = pd.DataFrame(index=[0], columns=[\"Fold\", \"Shrinkage\", \"Train RMSE\", \"Test RMSE\"])\n",
    "ridge_shrinkage = np.linspace(0.00001, 0.4, num=200)\n",
    "\n",
    "# main loop\n",
    "k, f = 0, 0\n",
    "for (train, test) in kf.split(df):\n",
    "    f += 1\n",
    "    # separate variables and folds\n",
    "    x_train = df.drop(labels=target, axis=1).values[train]\n",
    "    x_test = df.drop(labels=target, axis=1).values[test]\n",
    "    y_train = df[target].values[train]\n",
    "    y_test = df[target].values[test]\n",
    "    \n",
    "    # scale  variables\n",
    "    scaler_x = StandardScaler().fit(x_train)\n",
    "    x_train = np.hstack([np.ones((x_train.shape[0], 1)), scaler_x.transform(x_train)])\n",
    "    x_test = np.hstack([np.ones((x_test.shape[0], 1)), scaler_x.transform(x_test)])\n",
    "    \n",
    "    # fit model\n",
    "    for l in ridge_shrinkage:\n",
    "        # train model\n",
    "        inv_component = np.linalg.inv(np.matmul(x_train.transpose(), x_train) + np.eye(x_train.shape[1]) * l * x_train.shape[1])\n",
    "        coefs = np.matmul(inv_component, np.matmul(x_train.transpose(), y_train))\n",
    "        \n",
    "        # get predictions\n",
    "        pred_train = np.matmul(x_train, coefs)\n",
    "        pred_test = np.matmul(x_test, coefs)\n",
    "        \n",
    "        # compute metrics\n",
    "        rmse_train = np.sqrt(np.mean((y_train - pred_train) ** 2.0))\n",
    "        rmse_test = np.sqrt(np.mean((y_test - pred_test) ** 2.0))\n",
    "        \n",
    "        # store results\n",
    "        df_metrics.loc[k, \"Fold\"] = f\n",
    "        df_metrics.loc[k, \"Shrinkage\"] = l\n",
    "        df_metrics.loc[k, \"Train RMSE\"] = rmse_train\n",
    "        df_metrics.loc[k, \"Test RMSE\"] = rmse_test\n",
    "        k += 1\n",
    "        \n",
    "        # if using sklearn: from sklearn.linear_model import Ridge\n",
    "        #ml = Ridge(alpha=l).fit(x_train, y_train)\n",
    "        #pred_train = ml.predict(x_train)\n",
    "        #pred_test = ml.predict(x_test)\n",
    "\n",
    "# final organization\n",
    "df_metrics = df_metrics.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics = df_metrics.pivot_table(index=\"Shrinkage\", values=[\"Train RMSE\", \"Test RMSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics[[\"Test RMSE\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics.loc[df_agg_metrics[\"Test RMSE\"].idxmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Boston Housing with Polynomial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some new params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_params = {\"degree\": 3,\n",
    "               \"interaction_only\": False,\n",
    "               \"include_bias\": True\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_func = PolynomialFeatures(**poly_params).fit(df.drop(labels=target, axis=1))\n",
    "x_train = poly_func.transform(df.drop(labels=target, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels=target, axis=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting small - Polynomial Ridge regression - One variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-instantiation\n",
    "df_metrics = pd.DataFrame(index=[0], columns=[\"Fold\", \"Shrinkage\", \"Train RMSE\", \"Test RMSE\"])\n",
    "\n",
    "# main loop\n",
    "k, f = 0, 0\n",
    "for (train, test) in kf.split(df):\n",
    "    f += 1\n",
    "    # separate variables and folds\n",
    "    x_train = df_onevar.drop(labels=target, axis=1).values[train]\n",
    "    x_test = df_onevar.drop(labels=target, axis=1).values[test]\n",
    "    y_train = df_onevar[target].values[train]\n",
    "    y_test = df_onevar[target].values[test]\n",
    "    \n",
    "    # scale variables\n",
    "    scaler_x = StandardScaler().fit(x_train)\n",
    "    x_train = scaler_x.transform(x_train)\n",
    "    x_test = scaler_x.transform(x_test)\n",
    "    \n",
    "    # polynomial features - will include bias automatically\n",
    "    poly_params = {\"degree\": 12,\n",
    "                   \"interaction_only\": False,\n",
    "                   \"include_bias\": True\n",
    "                  }\n",
    "    poly_func = PolynomialFeatures(**poly_params).fit(x_train)\n",
    "    x_train = poly_func.transform(x_train)\n",
    "    x_test = poly_func.transform(x_test)\n",
    "        \n",
    "    # fit model\n",
    "    # train model\n",
    "    l = 100.0 # ridge shrinkage\n",
    "    inv_component = np.linalg.inv(np.matmul(x_train.transpose(), x_train) + np.eye(x_train.shape[1]) * l * x_train.shape[1])\n",
    "    coefs = np.matmul(inv_component, np.matmul(x_train.transpose(), y_train))\n",
    "\n",
    "    # get predictions\n",
    "    pred_train = np.matmul(x_train, coefs)\n",
    "    pred_test = np.matmul(x_test, coefs)\n",
    "\n",
    "    # compute metrics\n",
    "    rmse_train = np.sqrt(np.mean((y_train - pred_train) ** 2.0))\n",
    "    rmse_test = np.sqrt(np.mean((y_test - pred_test) ** 2.0))\n",
    "\n",
    "    # store results\n",
    "    df_metrics.loc[k, \"Fold\"] = f\n",
    "    df_metrics.loc[k, \"Shrinkage\"] = l\n",
    "    df_metrics.loc[k, \"Train RMSE\"] = rmse_train\n",
    "    df_metrics.loc[k, \"Test RMSE\"] = rmse_test\n",
    "    k += 1\n",
    "\n",
    "# final organization\n",
    "print(coefs)\n",
    "print(df_metrics.astype(float).mean())\n",
    "indices = np.argsort(x_train[:, 1])\n",
    "plt.scatter(x_train[indices, 1], y_train[indices])\n",
    "plt.plot(x_train[indices, 1], pred_train[indices], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop - All variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-instantiation\n",
    "poly_params = {\"degree\": 2,\n",
    "               \"interaction_only\": False,\n",
    "               \"include_bias\": True\n",
    "              }\n",
    "df_metrics = pd.DataFrame(index=[0], columns=[\"Fold\", \"Shrinkage\", \"Train RMSE\", \"Test RMSE\"])\n",
    "\n",
    "# main loop\n",
    "k, f = 0, 0\n",
    "for (train, test) in kf.split(df):\n",
    "    f += 1\n",
    "    # separate variables and folds\n",
    "    x_train = df.drop(labels=target, axis=1).values[train]\n",
    "    x_test = df.drop(labels=target, axis=1).values[test]\n",
    "    y_train = df[target].values[train]\n",
    "    y_test = df[target].values[test]\n",
    "    \n",
    "    # scale variables\n",
    "    scaler_x = StandardScaler().fit(x_train)\n",
    "    x_train = scaler_x.transform(x_train)\n",
    "    x_test = scaler_x.transform(x_test)\n",
    "    \n",
    "    # polynomial features - will include bias automatically\n",
    "    poly_func = PolynomialFeatures(**poly_params).fit(x_train)\n",
    "    x_train = poly_func.transform(x_train)\n",
    "    x_test = poly_func.transform(x_test)\n",
    "        \n",
    "    # fit model\n",
    "    for l in ridge_shrinkage:\n",
    "        # train model\n",
    "        inv_component = np.linalg.inv(np.matmul(x_train.transpose(), x_train) + np.eye(x_train.shape[1]) * l * x_train.shape[1])\n",
    "        coefs = np.matmul(inv_component, np.matmul(x_train.transpose(), y_train))\n",
    "        \n",
    "        # get predictions\n",
    "        pred_train = np.matmul(x_train, coefs)\n",
    "        pred_test = np.matmul(x_test, coefs)\n",
    "        \n",
    "        # compute metrics\n",
    "        rmse_train = np.sqrt(np.mean((y_train - pred_train) ** 2.0))\n",
    "        rmse_test = np.sqrt(np.mean((y_test - pred_test) ** 2.0))\n",
    "        \n",
    "        # store results\n",
    "        df_metrics.loc[k, \"Fold\"] = f\n",
    "        df_metrics.loc[k, \"Shrinkage\"] = l\n",
    "        df_metrics.loc[k, \"Train RMSE\"] = rmse_train\n",
    "        df_metrics.loc[k, \"Test RMSE\"] = rmse_test\n",
    "        k += 1\n",
    "        \n",
    "        # if using sklearn: from sklearn.linear_model import Ridge\n",
    "        #ml = Ridge(alpha=l).fit(x_train, y_train)\n",
    "        #pred_train = ml.predict(x_train)\n",
    "        #pred_test = ml.predict(x_test)\n",
    "\n",
    "# final organization\n",
    "df_metrics = df_metrics.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics = df_metrics.pivot_table(index=\"Shrinkage\", values=[\"Train RMSE\", \"Test RMSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics.loc[df_agg_metrics[\"Test RMSE\"].idxmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Boston Housing with Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_kernel(X, degree):\n",
    "    return (np.matmul(X, X.transpose())) ** degree\n",
    "    # return (np.matmul(X, X.transpose()) + 1.0) ** degree  # K(X, Y) = (gamma <X, Y> + coef0)^degree (sklearn implementation)\n",
    "    # return (np.eye(X.shape[0]) + np.matmul(X, X.transpose())) ** degree  # anova dot-kernel - try it later if you are interested\n",
    "\n",
    "# passing some parameters\n",
    "krr_degree = 5\n",
    "krr_shrinkage = np.linspace(0.00001, 0.4, num=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-instantiation\n",
    "df_metrics = pd.DataFrame(index=[0], columns=[\"Fold\", \"Shrinkage\", \"Train RMSE\", \"Test RMSE\"])\n",
    "\n",
    "# main loop\n",
    "k, f = 0, 0\n",
    "for (train, test) in kf.split(df):\n",
    "    f += 1\n",
    "    # separate variables and folds\n",
    "    x_train = df.drop(labels=target, axis=1).values[train]\n",
    "    x_test = df.drop(labels=target, axis=1).values[test]\n",
    "    y_train = df[target].values[train]\n",
    "    y_test = df[target].values[test]\n",
    "    \n",
    "    # scaling variables\n",
    "    scaler_x = StandardScaler().fit(x_train)\n",
    "    x_train = scaler_x.transform(x_train)\n",
    "    x_test = scaler_x.transform(x_test)\n",
    "    \n",
    "    # creating kernel matrix\n",
    "    x_data = np.hstack([np.ones((df.shape[0], 1)), np.vstack([x_train, x_test])])\n",
    "    # x_data = np.vstack([x_train, x_test]) # no intercept\n",
    "    K = poly_kernel(x_data, krr_degree)\n",
    "    \n",
    "    # splitting kernel matrix in training an test\n",
    "    # remember: K = |k(x_train, x_train), k(x_test, x_train) |\n",
    "    #               |k(x_train, x_test), k(x_test, x_test)   |\n",
    "    # we train using k(x_train, x_train) block, and test using k(x_train, x_test) block\n",
    "    k_train = K[:x_train.shape[0], :x_train.shape[0]]\n",
    "    k_test = K[x_train.shape[0]:, :x_train.shape[0]]\n",
    "        \n",
    "    # fit model\n",
    "    for l in krr_shrinkage:\n",
    "        # train model\n",
    "        inv_component = np.linalg.inv(k_train + np.eye(x_train.shape[0]) * l * x_train.shape[0])\n",
    "        coefs = np.matmul(inv_component, y_train)\n",
    "                \n",
    "        # get predictions\n",
    "        pred_train = np.matmul(k_train, coefs)\n",
    "        pred_test = np.matmul(k_test, coefs)\n",
    "        \n",
    "        # compute metrics\n",
    "        rmse_train = np.sqrt(np.mean((y_train - pred_train) ** 2.0))\n",
    "        rmse_test = np.sqrt(np.mean((y_test - pred_test) ** 2.0))\n",
    "        \n",
    "        # store results\n",
    "        df_metrics.loc[k, \"Fold\"] = f\n",
    "        df_metrics.loc[k, \"Shrinkage\"] = l\n",
    "        df_metrics.loc[k, \"Train RMSE\"] = rmse_train\n",
    "        df_metrics.loc[k, \"Test RMSE\"] = rmse_test\n",
    "        k += 1\n",
    "        \n",
    "        # if using sklearn: from sklearn.linear_model import Ridge\n",
    "        #ml = KernelRidge(kernel=\"poly\", degree=krr_degree, alpha=l).fit(x_train, y_train)\n",
    "        #pred_train = ml.predict(x_train)\n",
    "        #pred_test = ml.predict(x_test)\n",
    "\n",
    "# final organization\n",
    "df_metrics = df_metrics.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics = df_metrics.pivot_table(index=\"Shrinkage\", values=[\"Train RMSE\", \"Test RMSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics.loc[df_agg_metrics[\"Test RMSE\"].idxmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Boston Housing with Gaussian/RBF Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature space of polynomial or linear kernels are finite, whilst Gaussian/RBF kernel induces a feature space of infinite dimension: https://en.wikipedia.org/wiki/Radial_basis_function_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X, sigma):\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "    K = euclidean_distances(X, X, squared=True)\n",
    "    K *= -sigma\n",
    "    return np.exp(K, K)  # exponentiate K in-place\n",
    "    \n",
    "# passing some parameters\n",
    "krr_sigma = 0.5\n",
    "krr_shrinkage = np.linspace(0.00001, 0.4, num=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just with one variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-instantiation\n",
    "df_metrics = pd.DataFrame(index=[0], columns=[\"Fold\", \"Shrinkage\", \"Train RMSE\", \"Test RMSE\"])\n",
    "\n",
    "# main loop\n",
    "k, f = 0, 0\n",
    "for (train, test) in kf.split(df_onevar):\n",
    "    f += 1\n",
    "    # separate variables and folds\n",
    "    x_train = df_onevar.drop(labels=target, axis=1).values[train]\n",
    "    x_test = df_onevar.drop(labels=target, axis=1).values[test]\n",
    "    y_train = df_onevar[target].values[train]\n",
    "    y_test = df_onevar[target].values[test]\n",
    "    \n",
    "    # scaling variables\n",
    "    scaler_x = StandardScaler().fit(x_train)\n",
    "    x_train = scaler_x.transform(x_train)\n",
    "    x_test = scaler_x.transform(x_test)\n",
    "    \n",
    "    # creating kernel matrix\n",
    "    krr_sigma = .5\n",
    "    x_data = np.hstack([np.ones((df.shape[0], 1)), np.vstack([x_train, x_test])])\n",
    "    K = rbf_kernel(x_data, krr_sigma)\n",
    "    \n",
    "    # splitting kernel matrix in training an test\n",
    "    # remember: K = |k(x_train, x_train), k(x_test, x_train) |\n",
    "    #               |k(x_train, x_test), k(x_test, x_test)   |\n",
    "    # we train using k(x_train, x_train) block, and test using k(x_train, x_test) block\n",
    "    k_train = K[:x_train.shape[0], :x_train.shape[0]]\n",
    "    k_test = K[x_train.shape[0]:, :x_train.shape[0]]\n",
    "        \n",
    "    # fit model\n",
    "    # train model\n",
    "    l = 0.000010\n",
    "    inv_component = np.linalg.inv(k_train + np.eye(x_train.shape[0]) * l * x_train.shape[0])\n",
    "    coefs = np.matmul(inv_component, y_train)\n",
    "\n",
    "    # get predictions\n",
    "    pred_train = np.matmul(k_train, coefs)\n",
    "    pred_test = np.matmul(k_test, coefs)\n",
    "\n",
    "    # compute metrics\n",
    "    rmse_train = np.sqrt(np.mean((y_train - pred_train) ** 2.0))\n",
    "    rmse_test = np.sqrt(np.mean((y_test - pred_test) ** 2.0))\n",
    "\n",
    "    # store results\n",
    "    df_metrics.loc[k, \"Fold\"] = f\n",
    "    df_metrics.loc[k, \"Shrinkage\"] = l\n",
    "    df_metrics.loc[k, \"Train RMSE\"] = rmse_train\n",
    "    df_metrics.loc[k, \"Test RMSE\"] = rmse_test\n",
    "    k += 1\n",
    "\n",
    "# final organization\n",
    "#print(coefs)\n",
    "print(df_metrics.astype(float).mean())\n",
    "indices = np.argsort(x_train[:, 0])\n",
    "plt.scatter(x_train[indices, 0], y_train[indices])\n",
    "plt.plot(x_train[indices, 0], pred_train[indices], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-instantiation\n",
    "df_metrics = pd.DataFrame(index=[0], columns=[\"Fold\", \"Shrinkage\", \"Train RMSE\", \"Test RMSE\"])\n",
    "krr_shrinkage = np.linspace(0.00001, 0.4, num=200)\n",
    "\n",
    "# main loop\n",
    "k, f = 0, 0\n",
    "for (train, test) in kf.split(df):\n",
    "    f += 1\n",
    "    # separate variables and folds\n",
    "    x_train = df.drop(labels=target, axis=1).values[train]\n",
    "    x_test = df.drop(labels=target, axis=1).values[test]\n",
    "    y_train = df[target].values[train]\n",
    "    y_test = df[target].values[test]\n",
    "    \n",
    "    # scaling variables\n",
    "    scaler_x = StandardScaler().fit(x_train)\n",
    "    x_train = scaler_x.transform(x_train)\n",
    "    x_test = scaler_x.transform(x_test)\n",
    "    \n",
    "    # creating kernel matrix\n",
    "    x_data = np.hstack([np.ones((df.shape[0], 1)), np.vstack([x_train, x_test])])\n",
    "    krr_sigma = .05\n",
    "    K = rbf_kernel(x_data, krr_sigma)\n",
    "    \n",
    "    # splitting kernel matrix in training an test\n",
    "    # remember: K = |k(x_train, x_train), k(x_test, x_train) |\n",
    "    #               |k(x_train, x_test), k(x_test, x_test)   |\n",
    "    # we train using k(x_train, x_train) block, and test using k(x_train, x_test) block\n",
    "    k_train = K[:x_train.shape[0], :x_train.shape[0]]\n",
    "    k_test = K[x_train.shape[0]:, :x_train.shape[0]]\n",
    "        \n",
    "    # fit model\n",
    "    for l in krr_shrinkage:\n",
    "        # train model\n",
    "        inv_component = np.linalg.inv(k_train + np.eye(x_train.shape[0]) * l * x_train.shape[0])\n",
    "        coefs = np.matmul(inv_component, y_train)\n",
    "                \n",
    "        # get predictions\n",
    "        pred_train = np.matmul(k_train, coefs)\n",
    "        pred_test = np.matmul(k_test, coefs)\n",
    "        \n",
    "        # compute metrics\n",
    "        rmse_train = np.sqrt(np.mean((y_train - pred_train) ** 2.0))\n",
    "        rmse_test = np.sqrt(np.mean((y_test - pred_test) ** 2.0))\n",
    "        \n",
    "        # store results\n",
    "        df_metrics.loc[k, \"Fold\"] = f\n",
    "        df_metrics.loc[k, \"Shrinkage\"] = l\n",
    "        df_metrics.loc[k, \"Train RMSE\"] = rmse_train\n",
    "        df_metrics.loc[k, \"Test RMSE\"] = rmse_test\n",
    "        k += 1\n",
    "        \n",
    "        # if using sklearn: from sklearn.linear_model import Ridge\n",
    "        #ml = KernelRidge(kernel=\"rbf\", degree=krr_sigma, alpha=l).fit(x_train, y_train)\n",
    "        #pred_train = ml.predict(x_train)\n",
    "        #pred_test = ml.predict(x_test)\n",
    "\n",
    "# final organization\n",
    "df_metrics = df_metrics.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics = df_metrics.pivot_table(index=\"Shrinkage\", values=[\"Train RMSE\", \"Test RMSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics.loc[df_agg_metrics[\"Test RMSE\"].idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(coefs).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearly separable problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linearly separable problem\n",
    "class_plus1 = np.hstack([np.random.normal(loc=[100.0, 5.0], scale=[2.0, 1.0], size=(200, 2)), np.ones((200, 1))])\n",
    "class_minus1 = np.hstack([np.random.normal(loc=[50.0, -1.0], scale=[2.0, 1.0], size=(200, 2)), -np.ones((200, 1))])\n",
    "df = pd.DataFrame(np.vstack([class_plus1, class_minus1]), columns=[\"X\", \"Z\", \"Y\"])\n",
    "\n",
    "# plot\n",
    "plt.scatter(df[\"X\"], df[\"Z\"], c=df[\"Y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# not scaled\n",
    "svm_unscaled = Pipeline(( (\"scaler\", StandardScaler(with_mean=False, with_std=False)),\n",
    "                       (\"linear_svc\", LinearSVC(loss=\"hinge\", C=10.0 ** -10.0)),\n",
    "                      ))\n",
    "svm_unscaled = svm_unscaled.fit(df[[\"X\", \"Z\"]].values, df[\"Y\"].values)\n",
    "\n",
    "# with scaling\n",
    "svm_scaled = Pipeline(( (\"scaler\", StandardScaler()),\n",
    "                       (\"linear_svc\", LinearSVC(loss=\"hinge\", C=10.0 ** -10.0)),\n",
    "                      ))\n",
    "svm_scaled = svm_scaled.fit(df[[\"X\", \"Z\"]].values, df[\"Y\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charts - Sensitivity to feature scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# Instantiation\n",
    "x_min, x_max = df[\"X\"].min() - 1, df[\"X\"].max() + 1\n",
    "z_min, z_max = df[\"Z\"].min() - 1, df[\"Z\"].max() + 1\n",
    "xx, zz = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(z_min, z_max, 0.1))\n",
    "f, axarr = plt.subplots(nrows=1, ncols=2, sharex='col', sharey='row', figsize=(17, 7))\n",
    "\n",
    "# Plotting decision regions\n",
    "for idx, clf, tt in zip(product([0, 1]), [svm_unscaled, svm_scaled], [\"Unscaled Linear-SVM\", \"Scaled Linear-SVM\"]):\n",
    "\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), zz.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    axarr[idx[0]].contourf(xx, zz, Z, alpha=0.4)\n",
    "    Z = clf.predict(np.c_[xx.ravel(), zz.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    axarr[idx[0]].contourf(xx, zz, Z, alpha=0.4)\n",
    "    axarr[idx[0]].scatter(df[\"X\"].values, df[\"Z\"].values, c=df[\"Y\"].values, s=20, edgecolor='k')\n",
    "    axarr[idx[0]].set_title(tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linearly separable problem, but with outliers\n",
    "np.random.seed(seed=119)\n",
    "class_plus1 = np.hstack([np.random.normal(loc=[17.0, 5.0], scale=[3.0, 2.0], size=(200, 2)), np.ones((200, 1))])\n",
    "class_minus1 = np.hstack([np.random.normal(loc=[1.0, 1.0], scale=[3.0, 1.0], size=(200, 2)), -np.ones((200, 1))])\n",
    "df = pd.DataFrame(np.vstack([class_plus1, class_minus1]), columns=[\"X\", \"Z\", \"Y\"])\n",
    "\n",
    "# plot\n",
    "plt.scatter(df[\"X\"], df[\"Z\"], c=df[\"Y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# different penalties\n",
    "C = [0.0000001, 0.0001, 10.0]\n",
    "\n",
    "# different models\n",
    "svm_c = []\n",
    "for c in C:\n",
    "    svm_c.append(Pipeline(( (\"scaler\", StandardScaler()),\n",
    "                           (\"linear_svc\", LinearSVC(loss=\"hinge\", C=c)),\n",
    "                          ))\n",
    "                )\n",
    "    svm_c[-1].fit(df[[\"X\", \"Z\"]].values, df[\"Y\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charts - Fewer vs More margin violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# Instantiation\n",
    "x_min, x_max = df[\"X\"].min() - 1, df[\"X\"].max() + 1\n",
    "z_min, z_max = df[\"Z\"].min() - 1, df[\"Z\"].max() + 1\n",
    "xx, zz = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(z_min, z_max, 0.1))\n",
    "f, axarr = plt.subplots(nrows=1, ncols=3, sharex='col', sharey='row', figsize=(17, 5))\n",
    "\n",
    "# Plotting decision regions\n",
    "for idx, clf, tt in zip(product([0, 1, 2]), svm_c, [\"C = \" + str(c) for c in C]):\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), zz.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    axarr[idx[0]].contourf(xx, zz, Z, alpha=0.4)\n",
    "    axarr[idx[0]].scatter(df[\"X\"].values, df[\"Z\"].values, c=df[\"Y\"].values, s=20, edgecolor='k')\n",
    "    axarr[idx[0]].set_title(tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machines with Explicit Feature Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linearly separable problem, but with outliers\n",
    "from sklearn.datasets import make_moons\n",
    "np.random.seed(seed=119)\n",
    "df = make_moons(n_samples=400, noise=0.3, random_state=0)\n",
    "df = pd.DataFrame({\"X\": df[0][:, 0],\n",
    "                   \"Z\": df[0][:, 1],\n",
    "                   \"Y\": df[1]})\n",
    "\n",
    "# plot\n",
    "plt.scatter(df[\"X\"], df[\"Z\"], c=df[\"Y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# different penalties\n",
    "degree = [1, 3, 7]\n",
    "\n",
    "# different models\n",
    "svm_d = []\n",
    "for d in degree:\n",
    "    svm_d.append(Pipeline(((\"poly_features\", PolynomialFeatures(degree=d)), \n",
    "                           (\"scaler\", StandardScaler()),\n",
    "                           (\"linear_svc\", LinearSVC(loss=\"hinge\", C=0.10)),\n",
    "                          ))\n",
    "                )\n",
    "    svm_d[-1].fit(df[[\"X\", \"Z\"]].values, df[\"Y\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charts - Nonlinear decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# Instantiation\n",
    "x_min, x_max = df[\"X\"].min() - 1, df[\"X\"].max() + 1\n",
    "z_min, z_max = df[\"Z\"].min() - 1, df[\"Z\"].max() + 1\n",
    "xx, zz = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(z_min, z_max, 0.1))\n",
    "f, axarr = plt.subplots(nrows=1, ncols=3, sharex='col', sharey='row', figsize=(17, 5))\n",
    "\n",
    "# Plotting decision regions\n",
    "for idx, clf, tt in zip(product([0, 1, 2]), svm_d, [\"Degree = \" + str(d) for d in degree]):\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), zz.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    axarr[idx[0]].contourf(xx, zz, Z, alpha=0.4)\n",
    "    axarr[idx[0]].scatter(df[\"X\"].values, df[\"Z\"].values, c=df[\"Y\"].values, s=20, edgecolor='k')\n",
    "    axarr[idx[0]].set_title(tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset - Harder to separate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonlinearly separable problem\n",
    "np.random.seed(seed=119)\n",
    "class_data = np.random.normal(loc=[0.0, 0.0], scale=[1.0, 1.0], size=(400, 2))\n",
    "class_pos = (class_data[:, 0] ** 2.0 + class_data[:, 1] ** 2.0) < np.random.uniform(low=0.0, high=2., size=400)\n",
    "class_plus1 = class_data[class_pos, :]\n",
    "class_minus1 = class_data[class_pos == False, :]\n",
    "df = pd.DataFrame(np.vstack([np.hstack([class_plus1, np.ones((class_plus1.shape[0], 1))]), \n",
    "                            np.hstack([class_minus1, -np.ones((class_minus1.shape[0], 1))])]), columns=[\"X\", \"Z\", \"Y\"])\n",
    "\n",
    "# plot\n",
    "plt.scatter(df[\"X\"], df[\"Z\"], c=df[\"Y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# different penalties\n",
    "sigma = [1., 10.0, 100.0]\n",
    "\n",
    "# different models\n",
    "svm_s = []\n",
    "for s in sigma:\n",
    "    svm_s.append(Pipeline(((\"scaler\", StandardScaler()),\n",
    "                           (\"kernel_svc\", SVC(kernel=\"rbf\", gamma=s, C=10.0)),\n",
    "                          ))\n",
    "                )\n",
    "    svm_s[-1].fit(df[[\"X\", \"Z\"]].values, df[\"Y\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charts - Hard to separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# Instantiation\n",
    "x_min, x_max = df[\"X\"].min() - 1, df[\"X\"].max() + 1\n",
    "z_min, z_max = df[\"Z\"].min() - 1, df[\"Z\"].max() + 1\n",
    "xx, zz = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(z_min, z_max, 0.1))\n",
    "f, axarr = plt.subplots(nrows=1, ncols=3, sharex='col', sharey='row', figsize=(17, 5))\n",
    "\n",
    "# Plotting decision regions\n",
    "for idx, clf, tt in zip(product([0, 1, 2]), svm_s, [\"Sigma = \" + str(s) for s in sigma]):\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), zz.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    axarr[idx[0]].contourf(xx, zz, Z, alpha=0.4)\n",
    "    axarr[idx[0]].scatter(df[\"X\"].values, df[\"Z\"].values, c=df[\"Y\"].values, s=20, edgecolor='k')\n",
    "    axarr[idx[0]].set_title(tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(svm_s[0].predict(df[[\"X\", \"Z\"]]) == df[\"Y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Support Vector Machine in the Primal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homework: https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.129.3368&rep=rep1&type=pdf\n",
    "# produce 2-3 slides presenting the methodology in section 4.1 and algorithm 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "X = df[[\"X\", \"Z\"]].values\n",
    "Y = df[[\"Y\"]].values\n",
    "gaus_sigma = 1.00\n",
    "C = 10\n",
    "\n",
    "# pre-instantiation\n",
    "K = rbf_kernel(X, gaus_sigma)\n",
    "old_sv = np.zeros(df.shape[0]) == 1\n",
    "new_sv = np.ones(df.shape[0]) == 1\n",
    "sv_coefs = np.zeros((df.shape[0], 1))\n",
    "all_coefs = np.zeros((df.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "while (np.mean(np.abs(old_sv*1.0 - new_sv*1.0)) > 0.00001):\n",
    "    # new becomes old\n",
    "    old_sv = np.copy(new_sv)\n",
    "    \n",
    "    # first part of the algorithm - compute coefficients only for the support vectors\n",
    "    inv_component = np.linalg.inv(K[old_sv, :][:, old_sv] + np.eye(X.shape[0])[old_sv, :][:, old_sv] * 1/C)\n",
    "    sv_coefs = np.matmul(inv_component, Y[old_sv, :])\n",
    "    \n",
    "    # store the coefficients for the support vectors, all the rest are zeros\n",
    "    all_coefs[old_sv] = sv_coefs\n",
    "    all_coefs[old_sv==0] = 0\n",
    "    \n",
    "    # find new support vectors\n",
    "    new_sv = ((Y * np.matmul(K, all_coefs)) < 1.0).reshape(1, -1)[0]\n",
    "    \n",
    "    # proportion of support vectors in the data\n",
    "    print(new_sv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.sign(np.matmul(K, all_coefs)) == Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
