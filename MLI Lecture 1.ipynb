{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to say to jupyter notebook to plot/show the charts on it \n",
    "%matplotlib inline \n",
    "import pandas as pd # data frames\n",
    "import numpy as np # matrix algebra\n",
    "from matplotlib import pyplot as plt # normal charts\n",
    "import seaborn as sns # fancy charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/askoshiyama/mli-cohort/master/boston.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining vars\n",
    "input_vars = [\"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\", \"V10\", \"V11\", \"V12\", \"V13\"]\n",
    "output_var = [\"T1\"]\n",
    "\n",
    "# dictionary of variables\n",
    "pd.read_table(\"https://raw.githubusercontent.com/askoshiyama/mli-cohort3/master/Boston%20-%20Dictionary%20of%20Variables.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some \"integrity\" checkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().loc[[\"T1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(y=output_var, x=\"V2\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(df[\"V13\"], df[\"T1\"], kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 5-fold cv\n",
    "k_folds = KFold(n_splits=5, random_state=10, shuffle=True)\n",
    "\n",
    "# performance metrics\n",
    "# homework: https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n",
    "def mse_score(y_true, y_pred):\n",
    "    return np.mean( (y_true - y_pred) ** 2.0)\n",
    "\n",
    "def rmse_score(y_true, y_pred):\n",
    "    return np.sqrt(mse_score(y_true, y_pred))\n",
    "\n",
    "def mad_score(y_true, y_pred):\n",
    "    return np.mean( np.abs(y_true - y_pred) )\n",
    "\n",
    "def pr2_score(y_true, y_pred):\n",
    "    return np.corrcoef(y_true, y_pred)[0, 1] ** 2.0\n",
    "\n",
    "#def pr2_score(y_true, y_pred):\n",
    "#    return (np.cov(y_true, y_pred)[0, 1] / (np.std(y_true) * np.std(y_pred))) ** 2.0\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    y_bar = np.mean(y_true)\n",
    "    return 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - y_bar)**2)\n",
    "\n",
    "#def r2_score(y_true, y_pred):\n",
    "#    y_bar = np.mean(y_true)\n",
    "#    return np.sum((y_pred - y_bar)**2) / np.sum((y_true - y_bar)**2)\n",
    "\n",
    "def adjpr2_score(y_true, y_pred, p):\n",
    "    return ( np.corrcoef(y_true, y_pred)[0, 1] ** 2.0 ) * ((y_true.shape[0] - p)/y_true.shape[0])\n",
    "\n",
    "perf_metrics = {\"MSE\": mse_score, \n",
    "                \"RMSE\": rmse_score, \n",
    "                \"Pseudo-R2\": pr2_score,\n",
    "                \"R2\": r2_score,\n",
    "                \"MAD\": mad_score\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy/Baseline Model - Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-instantiation\n",
    "target = [\"T1\"]\n",
    "df_metrics = pd.DataFrame(index=[0], columns=[\"Fold\", \"Metric\", \"Train\", \"Test\"])\n",
    "\n",
    "# main loop\n",
    "k, f = 0, 0\n",
    "for (train, test) in k_folds.split(df.drop(labels=target, axis=1)):\n",
    "    f += 1\n",
    "    print(f)\n",
    "    # separate variables and folds\n",
    "    x_train = df.drop(labels=target, axis=1).values[train]\n",
    "    x_test = df.drop(labels=target, axis=1).values[test]\n",
    "    y_train = df[target].values[train]\n",
    "    y_test = df[target].values[test]\n",
    "    \n",
    "    # \"fit\" model\n",
    "    model_pred = y_train.mean()\n",
    "\n",
    "    # get predictions\n",
    "    y_train_pred = np.repeat(model_pred, y_train.shape[0])\n",
    "    y_test_pred = np.repeat(model_pred, y_test.shape[0])\n",
    "\n",
    "    # compute metrics\n",
    "    for pf in list(perf_metrics.keys()):\n",
    "        df_metrics.loc[k, \"Fold\"] = f\n",
    "        df_metrics.loc[k, \"Metric\"] = pf\n",
    "        df_metrics.loc[k, \"Train\"] = perf_metrics[pf](y_train.ravel(), y_train_pred.ravel())\n",
    "        df_metrics.loc[k, \"Test\"] = perf_metrics[pf](y_test.ravel(), y_test_pred.ravel())\n",
    "        k += 1\n",
    "        \n",
    "    # if using sklearn: from from sklearn.dummy import DummyRegressor\n",
    "    #ml = DummyRegressor().fit(x_train, y_train)\n",
    "    #pred_train = ml.predict(x_train)\n",
    "    #pred_test = ml.predict(x_test)\n",
    "\n",
    "# final organization\n",
    "df_metrics = df_metrics.apply(pd.to_numeric, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.pivot_table(index=\"Metric\", aggfunc=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-instantiation\n",
    "target = [\"T1\"]\n",
    "df_metrics = pd.DataFrame(index=[0], columns=[\"Fold\", \"Metric\", \"Train\", \"Test\"])\n",
    "\n",
    "# main loop\n",
    "k, f = 0, 0\n",
    "for (train, test) in k_folds.split(df.drop(labels=target, axis=1)):\n",
    "    f += 1\n",
    "    print(f)\n",
    "    # separate variables and folds\n",
    "    x_train = df.drop(labels=target, axis=1).values[train]\n",
    "    x_train = np.hstack([np.ones((x_train.shape[0], 1)), x_train])\n",
    "    \n",
    "    x_test = df.drop(labels=target, axis=1).values[test]\n",
    "    x_test = np.hstack([np.ones((x_test.shape[0], 1)), x_test])\n",
    "    \n",
    "    y_train = df[target].values[train]\n",
    "    y_test = df[target].values[test]\n",
    "    \n",
    "    # fit model\n",
    "    # train model - coefs = (X_train'X_train)^-1 X_train'y_train\n",
    "    inv_component = np.linalg.pinv(np.matmul(x_train.transpose(), x_train))\n",
    "    coefs = np.matmul(inv_component, np.matmul(x_train.transpose(), y_train))\n",
    "\n",
    "    # get predictions\n",
    "    y_train_pred = np.matmul(x_train, coefs) # X_train*coefs\n",
    "    y_test_pred = np.matmul(x_test, coefs) # X_test*coefs\n",
    "\n",
    "    # compute metrics\n",
    "    for pf in list(perf_metrics.keys()):\n",
    "        df_metrics.loc[k, \"Fold\"] = f\n",
    "        df_metrics.loc[k, \"Metric\"] = pf\n",
    "        df_metrics.loc[k, \"Train\"] = perf_metrics[pf](y_train.ravel(), y_train_pred.ravel())\n",
    "        df_metrics.loc[k, \"Test\"] = perf_metrics[pf](y_test.ravel(), y_test_pred.ravel())\n",
    "        k += 1\n",
    "        \n",
    "    # if using sklearn: from sklearn.linear_model import LinearRegression\n",
    "    #ml = LinearRegression().fit(x_train, y_train)\n",
    "    #pred_train = ml.predict(x_train)\n",
    "    #pred_test = ml.predict(x_test)\n",
    "\n",
    "# final organization\n",
    "df_metrics = df_metrics.apply(pd.to_numeric, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.pivot_table(index=\"Metric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_params = {\"degree\": 2,\n",
    "               \"interaction_only\": False,\n",
    "               \"include_bias\": True\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_func = PolynomialFeatures(**poly_params).fit(df.drop(labels=target, axis=1))\n",
    "x_train = poly_func.transform(df.drop(labels=target, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-instantiation\n",
    "target = [\"T1\"]\n",
    "df_metrics = pd.DataFrame(index=[0], columns=[\"Fold\", \"Metric\", \"Train\", \"Test\"])\n",
    "\n",
    "# main loop\n",
    "k, f = 0, 0\n",
    "for (train, test) in k_folds.split(df.drop(labels=target, axis=1)):\n",
    "    f += 1\n",
    "    print(f)\n",
    "    # separate variables and folds\n",
    "    x_train = df.drop(labels=target, axis=1).values[train]\n",
    "    x_test = df.drop(labels=target, axis=1).values[test]\n",
    "    y_train = df[target].values[train]\n",
    "    y_test = df[target].values[test]\n",
    "    \n",
    "    # polynomial features - will include bias automatically\n",
    "    poly_func = PolynomialFeatures(**poly_params).fit(x_train)\n",
    "    x_train = poly_func.transform(x_train)\n",
    "    x_test = poly_func.transform(x_test)\n",
    "        \n",
    "    # fit model\n",
    "    # train model \n",
    "    inv_component = np.linalg.pinv(np.matmul(x_train.transpose(), x_train))\n",
    "    coefs = np.matmul(inv_component, np.matmul(x_train.transpose(), y_train))\n",
    "\n",
    "    # get predictions\n",
    "    y_train_pred = np.matmul(x_train, coefs)\n",
    "    y_test_pred = np.matmul(x_test, coefs)\n",
    "\n",
    "    # compute metrics\n",
    "    for pf in list(perf_metrics.keys()):\n",
    "        df_metrics.loc[k, \"Fold\"] = f\n",
    "        df_metrics.loc[k, \"Metric\"] = pf\n",
    "        df_metrics.loc[k, \"Train\"] = perf_metrics[pf](y_train.ravel(), y_train_pred.ravel())\n",
    "        df_metrics.loc[k, \"Test\"] = perf_metrics[pf](y_test.ravel(), y_test_pred.ravel())\n",
    "        k += 1\n",
    "\n",
    "    # if using sklearn: from sklearn.linear_model import LinearRegression\n",
    "    #ml = LinearRegression().fit(x_train, y_train)\n",
    "    #pred_train = ml.predict(x_train)\n",
    "    #pred_test = ml.predict(x_test)\n",
    "\n",
    "# final organization\n",
    "df_metrics = df_metrics.apply(pd.to_numeric, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.pivot_table(index=\"Metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-3.95/9.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression with Backward Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-instantiation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "poly_params = {\"degree\": 2,\n",
    "               \"interaction_only\": False,\n",
    "               \"include_bias\": True\n",
    "              }\n",
    "target = [\"T1\"]\n",
    "df_metrics = pd.DataFrame(index=[0], columns=[\"Fold\", \"Metric\", \"Train\", \"Test\"])\n",
    "\n",
    "# main loop\n",
    "k, f = 0, 0\n",
    "for (train, test) in k_folds.split(df.drop(labels=target, axis=1)):\n",
    "    f += 1\n",
    "    print(f)\n",
    "    # separate variables and folds\n",
    "    x_train = df.drop(labels=target, axis=1).values[train]\n",
    "    x_test = df.drop(labels=target, axis=1).values[test]\n",
    "    y_train = df[target].values[train]\n",
    "    y_test = df[target].values[test]\n",
    "    \n",
    "    # polynomial features - will include bias automatically\n",
    "    poly_func = PolynomialFeatures(**poly_params).fit(x_train)\n",
    "    x_train = poly_func.transform(x_train)\n",
    "    x_test = poly_func.transform(x_test)\n",
    "        \n",
    "    # fit model\n",
    "    model = RFE(LinearRegression()).fit(x_train, y_train)\n",
    "    #model = RFECV(model, step=1, cv=5).fit(x_train, y_train)\n",
    "\n",
    "    # get predictions\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    # compute metrics\n",
    "    for pf in list(perf_metrics.keys()):\n",
    "        df_metrics.loc[k, \"Fold\"] = f\n",
    "        df_metrics.loc[k, \"Metric\"] = pf\n",
    "        df_metrics.loc[k, \"Train\"] = perf_metrics[pf](y_train.ravel(), y_train_pred.ravel())\n",
    "        df_metrics.loc[k, \"Test\"] = perf_metrics[pf](y_test.ravel(), y_test_pred.ravel())\n",
    "        k += 1\n",
    "\n",
    "# final organization\n",
    "df_metrics = df_metrics.apply(pd.to_numeric, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[:, model.support_].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.pivot_table(index=\"Metric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-instantiation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "poly_params = {\"degree\": 2,\n",
    "               \"interaction_only\": False,\n",
    "               \"include_bias\": True\n",
    "              }\n",
    "target = [\"T1\"]\n",
    "df_metrics = pd.DataFrame(index=[0], columns=[\"Fold\", \"Metric\", \"Train\", \"Test\"])\n",
    "\n",
    "# main loop\n",
    "k, f = 0, 0\n",
    "for (train, test) in k_folds.split(df.drop(labels=target, axis=1)):\n",
    "    f += 1\n",
    "    print(f)\n",
    "    # separate variables and folds\n",
    "    x_train = df.drop(labels=target, axis=1).values[train]\n",
    "    x_test = df.drop(labels=target, axis=1).values[test]\n",
    "    y_train = df[target].values[train]\n",
    "    y_test = df[target].values[test]\n",
    "        \n",
    "    # fit model\n",
    "    model = Pipeline([(\"Poly\", PolynomialFeatures(**poly_params)),\n",
    "                      (\"LR-BE\", RFE(LinearRegression()))])\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # get predictions\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    # compute metrics\n",
    "    for pf in list(perf_metrics.keys()):\n",
    "        df_metrics.loc[k, \"Fold\"] = f\n",
    "        df_metrics.loc[k, \"Metric\"] = pf\n",
    "        df_metrics.loc[k, \"Train\"] = perf_metrics[pf](y_train.ravel(), y_train_pred.ravel())\n",
    "        df_metrics.loc[k, \"Test\"] = perf_metrics[pf](y_test.ravel(), y_test_pred.ravel())\n",
    "        k += 1\n",
    "\n",
    "# final organization\n",
    "df_metrics = df_metrics.apply(pd.to_numeric, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.pivot_table(index=\"Metric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# pre-instantiation\n",
    "ridge_shrinkage = np.linspace(-0.1, 0.4, num=200)\n",
    "target = [\"T1\"]\n",
    "df_metrics = pd.DataFrame(index=[0], columns=[\"Fold\", \"Shrinkage\", \"Metric\", \"Train\", \"Test\"])\n",
    "\n",
    "# main loop\n",
    "k, f = 0, 0\n",
    "for (train, test) in k_folds.split(df.drop(labels=target, axis=1)):\n",
    "    f += 1\n",
    "    print(f)\n",
    "    # separate variables and folds\n",
    "    x_train = df.drop(labels=target, axis=1).values[train]\n",
    "    x_test = df.drop(labels=target, axis=1).values[test]\n",
    "    y_train = df[target].values[train]\n",
    "    y_test = df[target].values[test]\n",
    "    \n",
    "    # scale  variables\n",
    "    scaler_x = StandardScaler(with_mean=True, with_std=True).fit(x_train)\n",
    "    x_train = np.hstack([np.ones((x_train.shape[0], 1)), scaler_x.transform(x_train)])\n",
    "    x_test = np.hstack([np.ones((x_test.shape[0], 1)), scaler_x.transform(x_test)])\n",
    "    \n",
    "    # fit model\n",
    "    for l in ridge_shrinkage:\n",
    "        # train model - coefs = (X'X + I * \\lambda * p)^-1 X'y\n",
    "        inv_component = np.linalg.pinv(np.matmul(x_train.transpose(), x_train) + np.eye(x_train.shape[1]) * l * x_train.shape[1])\n",
    "        coefs = np.matmul(inv_component, np.matmul(x_train.transpose(), y_train))\n",
    "        \n",
    "        # get predictions\n",
    "        y_train_pred = np.matmul(x_train, coefs) # X * coefs\n",
    "        y_test_pred = np.matmul(x_test, coefs)\n",
    "        \n",
    "        # compute metrics\n",
    "        for pf in list(perf_metrics.keys()):\n",
    "            df_metrics.loc[k, \"Fold\"] = f\n",
    "            df_metrics.loc[k, \"Metric\"] = pf\n",
    "            df_metrics.loc[k, \"Shrinkage\"] = l\n",
    "            df_metrics.loc[k, \"Train\"] = perf_metrics[pf](y_train.ravel(), y_train_pred.ravel())\n",
    "            df_metrics.loc[k, \"Test\"] = perf_metrics[pf](y_test.ravel(), y_test_pred.ravel())\n",
    "            k += 1\n",
    "        \n",
    "        # if using sklearn: from sklearn.linear_model import Ridge\n",
    "        #ml = Ridge(alpha=l).fit(x_train, y_train)\n",
    "        #pred_train = ml.predict(x_train)\n",
    "        #pred_test = ml.predict(x_test)\n",
    "\n",
    "# final organization\n",
    "df_metrics = df_metrics.apply(pd.to_numeric, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.pivot_table(index=\"Shrinkage\", columns=\"Metric\", values=\"Test\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics = df_metrics.pivot_table(index=\"Shrinkage\", columns=\"Metric\", values=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics[[\"RMSE\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics.loc[df_agg_metrics[\"Pseudo-R2\"].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge + Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-instantiation\n",
    "from sklearn.linear_model import Ridge\n",
    "poly_params = {\"degree\": 2,\n",
    "               \"interaction_only\": False,\n",
    "               \"include_bias\": False\n",
    "              }\n",
    "ridge_shrinkage = np.linspace(8.0, 14.0, num=200)\n",
    "target = [\"T1\"]\n",
    "df_metrics = pd.DataFrame(index=[0], columns=[\"Fold\", \"Shrinkage\", \"Metric\", \"Train\", \"Test\"])\n",
    "\n",
    "# main loop\n",
    "k, f = 0, 0\n",
    "for (train, test) in k_folds.split(df.drop(labels=target, axis=1)):\n",
    "    f += 1\n",
    "    print(f)\n",
    "    # separate variables and folds\n",
    "    x_train = df.drop(labels=target, axis=1).values[train]\n",
    "    x_test = df.drop(labels=target, axis=1).values[test]\n",
    "    y_train = df[target].values[train]\n",
    "    y_test = df[target].values[test]\n",
    "    \n",
    "    # scale variables\n",
    "    #scaler_x = StandardScaler().fit(x_train)\n",
    "    #x_train = scaler_x.transform(x_train)\n",
    "    #x_test = scaler_x.transform(x_test)\n",
    "    \n",
    "    # polynomial features - will include bias automatically\n",
    "    #poly_func = PolynomialFeatures(**poly_params).fit(x_train)\n",
    "    #x_train = poly_func.transform(x_train)\n",
    "    #x_test = poly_func.transform(x_test)\n",
    "            \n",
    "    # fit model\n",
    "    for l in ridge_shrinkage:\n",
    "        # train model - min ||coefs||^2, subject to: X coefs = y -- L(\\lambda) = ||coefs||^1 + \\lambda * (X coefs - y)\n",
    "        #inv_component = np.linalg.pinv(np.matmul(x_train.transpose(), x_train) + np.eye(x_train.shape[1]) * l * x_train.shape[1])\n",
    "        #coefs = np.matmul(inv_component, np.matmul(x_train.transpose(), y_train))\n",
    "        \n",
    "        # fit model\n",
    "        model = Pipeline([(\"StdScaler\", StandardScaler()),\n",
    "                          (\"Poly\", PolynomialFeatures(**poly_params)),\n",
    "                          (\"Ridge\", Ridge(alpha=l))])\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        # get predictions\n",
    "        #y_train_pred = np.matmul(x_train, coefs)\n",
    "        y_train_pred = model.predict(x_train)\n",
    "        #y_test_pred = np.matmul(x_test, coefs)\n",
    "        y_test_pred = model.predict(x_test)\n",
    "        \n",
    "        # compute metrics\n",
    "        for pf in list(perf_metrics.keys()):\n",
    "            df_metrics.loc[k, \"Fold\"] = f\n",
    "            df_metrics.loc[k, \"Metric\"] = pf\n",
    "            df_metrics.loc[k, \"Shrinkage\"] = l\n",
    "            df_metrics.loc[k, \"Train\"] = perf_metrics[pf](y_train.ravel(), y_train_pred.ravel())\n",
    "            df_metrics.loc[k, \"Test\"] = perf_metrics[pf](y_test.ravel(), y_test_pred.ravel())\n",
    "            k += 1\n",
    "        \n",
    "        # if using sklearn: from sklearn.linear_model import Ridge, Lasso\n",
    "        #ml = Ridge(alpha=l).fit(x_train, y_train)\n",
    "        #pred_train = ml.predict(x_train)\n",
    "        #pred_test = ml.predict(x_test)\n",
    "        \n",
    "        \n",
    "\n",
    "# final organization\n",
    "df_metrics = df_metrics.apply(pd.to_numeric, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics = df_metrics.pivot_table(index=\"Shrinkage\", columns=\"Metric\", values=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics[[\"RMSE\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_metrics.loc[df_agg_metrics[\"RMSE\"].idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3.46/9.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/askoshiyama/mli-cohort/master/german_credit.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical variables\n",
    "cat_variables = ['account_check_status', 'credit_history', 'purpose', 'savings', 'present_emp_since', 'personal_status_sex',\n",
    "                'property', 'other_installment_plans', 'housing', 'job', 'telephone', \"other_debtors\", 'foreign_worker']\n",
    "\n",
    "# target variable\n",
    "output_variable = [\"default\"]\n",
    "\n",
    "# other integer variables\n",
    "int_variables = ['credits_this_bank', 'present_res_since', 'duration_in_month', 'people_under_maintenance', \n",
    "                 'installment_as_income_perc', 'age', 'credit_amount']\n",
    "# list(set(df.columns) - set(output_variable) - set(cat_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-allocation\n",
    "df_cat = pd.DataFrame(index=df.index)\n",
    "\n",
    "# one-hot encoding of categorical variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# I will do a loop for pedagogical reasons, but it is not entirely necessary\n",
    "for cat in cat_variables:\n",
    "    # one-hot encoding fitting\n",
    "    one_hot_func = OneHotEncoder().fit(df[[cat]])\n",
    "    \n",
    "    # mapping\n",
    "    cat_mapped = one_hot_func.transform(df[[cat]]).toarray()\n",
    "    \n",
    "    # storing\n",
    "    for (k, cat_label) in enumerate(one_hot_func.categories_[0]):\n",
    "        df_cat[cat + \"_\" + cat_label] = cat_mapped[:, k]\n",
    "\n",
    "# quick check\n",
    "df_cat.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bracketing integer variable - age\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "kbin_func = KBinsDiscretizer(n_bins=3, encode='onehot', strategy='quantile').fit(df[[\"age\"]])\n",
    "df_age = pd.DataFrame(kbin_func.transform(df[[\"age\"]]).toarray(), columns=[\"young\", \"adult\", \"senior\"])\n",
    "\n",
    "# checking\n",
    "pd.concat([df_age, df[[\"age\"]]], axis=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other types of transformations possible - log transformation \n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "log_mapping = FunctionTransformer(func=np.log, inverse_func=np.exp)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax1.hist(df[\"credit_amount\"])\n",
    "ax1.set_title(\"Pre-transformation\")\n",
    "\n",
    "ax2.hist(log_mapping.transform(df[[\"credit_amount\"]].values))\n",
    "ax2.set_title(\"After log-transformation\")\n",
    "df_log = pd.DataFrame(log_mapping.transform(df[[\"credit_amount\"]].values), columns=[\"log(credit)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidating a final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pd.concat([df[int_variables[:-2]], df_cat, df_age, df_log, df[output_variable]], axis=1)\n",
    "df.shape, df_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model[output_variable].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 3-fold cv\n",
    "k_folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=10)\n",
    "\n",
    "# performance metrics - \n",
    "# homework: https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics\n",
    "# homework: https://en.wikipedia.org/wiki/Confusion_matrix, \n",
    "perf_metrics = {\"Accuracy\": metrics.accuracy_score, \n",
    "                \"Precision\": metrics.precision_score, \n",
    "                \"Recall\": metrics.recall_score,\n",
    "                \"AUC\": metrics.roc_auc_score, \n",
    "                \"F1-Score\": metrics.f1_score, \n",
    "                \"Brier\": metrics.brier_score_loss\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - From \"Scratch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_prediction(X, theta):\n",
    "    z = np.dot(X, theta)\n",
    "    h = sigmoid(z)\n",
    "    return h\n",
    "\n",
    "def loss(h, y):\n",
    "    h = np.clip(h, 1e-15, 1 - 1e-15) # too close to zero or one\n",
    "    return -(y * np.log(h) + (1 - y) * np.log(1 - h)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test\n",
    "theta = np.random.normal(size=df_model.drop(labels=output_variable, axis=1).shape[1])\n",
    "pred = logistic_prediction(df_model.drop(labels=output_variable, axis=1).values, theta)\n",
    "loss(pred, df_model[output_variable].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop\n",
    "# pre-instantiation\n",
    "n_vars = df_model.drop(labels=output_variable, axis=1).shape[1] # number of variables\n",
    "step_size = 0.01 # finite difference step-size\n",
    "lr = 0.01 # learning rate -- dictate the speed\n",
    "max_iter = 30 # maximum number of iterations\n",
    "df_metrics = pd.DataFrame(index=[0], columns=[\"Fold\", \"Metric\", \"Train\", \"Test\"])\n",
    "\n",
    "# main loop\n",
    "k, f = 0, 0\n",
    "for (train, test) in k_folds.split(df_model.drop(labels=output_variable, axis=1), df_model[output_variable]):\n",
    "    f += 1\n",
    "    # separate variables and folds\n",
    "    x_train = df_model.drop(labels=output_variable, axis=1).values[train]\n",
    "    x_test = df_model.drop(labels=output_variable, axis=1).values[test]\n",
    "    y_train = df_model[output_variable].values[train]\n",
    "    y_test = df_model[output_variable].values[test]\n",
    "    \n",
    "    # scale variables\n",
    "    #scaler_x = StandardScaler().fit(x_train)\n",
    "    #x_train = np.hstack([np.ones((x_train.shape[0], 1)), scaler_x.transform(x_train)])\n",
    "    #x_test = np.hstack([np.ones((x_test.shape[0], 1)), scaler_x.transform(x_test)])\n",
    "    \n",
    "    # fit model\n",
    "    # initialization\n",
    "    theta = np.random.normal(size=n_vars) # random coefs\n",
    "    pred = logistic_prediction(x_train, theta)\n",
    "    loss_iter = loss(pred, y_train)\n",
    "    print(\"fold: %d, iter: %d, loss: %.4f\" % (f, 0, loss_iter), end=\"\\r\")\n",
    "    \n",
    "    # training\n",
    "    for it in range(1, max_iter):\n",
    "        log_grad = np.zeros(shape=(n_vars,))\n",
    "        # compute numerical gradient - finite differences\n",
    "        for c in range(n_vars):\n",
    "            new_theta = np.copy(theta)\n",
    "            new_theta[c] = new_theta[c] + step_size\n",
    "            new_pred = logistic_prediction(x_train, new_theta)\n",
    "            c_loss = loss(new_pred, y_train)\n",
    "            log_grad[c] = (c_loss - loss_iter)/(step_size) # f'(x) ~ (f(x+e) - f(x-e))/2*e\n",
    "        \n",
    "        # update model\n",
    "        theta = theta - lr * log_grad\n",
    "        \n",
    "        # get current loss\n",
    "        pred = logistic_prediction(x_train, theta)\n",
    "        diff_loss = np.abs(loss_iter - loss(pred, y_train))\n",
    "        loss_iter = loss(pred, y_train)\n",
    "        print(\"fold: %d, iter: %d, loss: %.4f, grad_norm: %.4f\" % (f, it, loss_iter, np.linalg.norm(log_grad)), end=\"\\r\")\n",
    "        if diff_loss < 1e-3:\n",
    "            break\n",
    "    \n",
    "    # compute final predictions\n",
    "    y_train_pred = logistic_prediction(x_train, theta)\n",
    "    y_test_pred = logistic_prediction(x_test, theta)\n",
    "    \n",
    "    # store results\n",
    "    for pf in list(perf_metrics.keys()):\n",
    "        df_metrics.loc[k, \"Fold\"] = f\n",
    "        df_metrics.loc[k, \"Metric\"] = pf\n",
    "        if pf in [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]: # put a threshold\n",
    "            df_metrics.loc[k, \"Train\"] = perf_metrics[pf](y_train, y_train_pred > 0.5)\n",
    "            df_metrics.loc[k, \"Test\"] = perf_metrics[pf](y_test, y_test_pred > 0.5)\n",
    "        else:\n",
    "            df_metrics.loc[k, \"Train\"] = perf_metrics[pf](y_train, y_train_pred)\n",
    "            df_metrics.loc[k, \"Test\"] = perf_metrics[pf](y_test, y_test_pred)\n",
    "\n",
    "        k += 1\n",
    "        \n",
    "        # if using sklearn: from sklearn.linear_model import Ridge\n",
    "        #ml = LogisticRegression(penalty=\"None\").fit(x_train, y_train)\n",
    "        #y_train_pred = ml.predict(x_train)\n",
    "        #y_test_pred = ml.predict(x_test)\n",
    "\n",
    "# final organization\n",
    "df_metrics = df_metrics.apply(pd.to_numeric, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.pivot_table(index=\"Metric\", values=[\"Train\", \"Test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Logistic Regression (a.k.a. Ridge Logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-instantiation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "shrink = np.linspace(0.001, 2.0, num=100)\n",
    "df_metrics = pd.DataFrame(index=[0], columns=[\"Fold\", \"Shrinkage\", \"Metric\", \"Train\", \"Test\"])\n",
    "\n",
    "# main loop\n",
    "k, f = 0, 0\n",
    "for (train, test) in k_folds.split(df_model.drop(labels=output_variable, axis=1), df_model[output_variable]):\n",
    "    f += 1\n",
    "    # separate variables and folds\n",
    "    x_train = df_model.drop(labels=output_variable, axis=1).values[train]\n",
    "    x_test = df_model.drop(labels=output_variable, axis=1).values[test]\n",
    "    y_train = df_model[output_variable].values[train]\n",
    "    y_test = df_model[output_variable].values[test]\n",
    "    \n",
    "    # scale  variables\n",
    "    scaler_x = StandardScaler().fit(x_train)\n",
    "    x_train = scaler_x.transform(x_train)\n",
    "    x_test = scaler_x.transform(x_test)\n",
    "    \n",
    "    # fit model\n",
    "    for l in shrink:\n",
    "        # train model\n",
    "        model = LogisticRegression(penalty=\"l2\", C=l, solver=\"lbfgs\").fit(x_train, y_train.ravel())\n",
    "        \n",
    "        # get predictions\n",
    "        y_train_pred = model.predict(x_train)\n",
    "        y_test_pred = model.predict(x_test)\n",
    "        \n",
    "        # compute metrics\n",
    "        for pf in list(perf_metrics.keys()):\n",
    "            df_metrics.loc[k, \"Fold\"] = f\n",
    "            df_metrics.loc[k, \"Metric\"] = pf\n",
    "            df_metrics.loc[k, \"Shrinkage\"] = l\n",
    "            if pf in [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]: # put a threshold\n",
    "                df_metrics.loc[k, \"Train\"] = perf_metrics[pf](y_train, y_train_pred > 0.5)\n",
    "                df_metrics.loc[k, \"Test\"] = perf_metrics[pf](y_test, y_test_pred > 0.5)\n",
    "            else:\n",
    "                df_metrics.loc[k, \"Train\"] = perf_metrics[pf](y_train, y_train_pred)\n",
    "                df_metrics.loc[k, \"Test\"] = perf_metrics[pf](y_test, y_test_pred)\n",
    "\n",
    "            k += 1\n",
    "    print(\"fold: %d\" % (f))\n",
    "\n",
    "# final organization\n",
    "df_metrics = df_metrics.apply(pd.to_numeric, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.pivot_table(index=[\"Shrinkage\"], columns=[\"Metric\"], values=[\"Test\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.pivot_table(index=[\"Shrinkage\"], columns=[\"Metric\"], values=[\"Test\"])[\"Test\"][\"Accuracy\"].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
